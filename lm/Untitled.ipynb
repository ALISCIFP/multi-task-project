{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Utilities for parsing CONll text files.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "#import pandas as pd\n",
    "import csv\n",
    "import pdb\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "#from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\"\"\"\n",
    "==========\n",
    "Section 1.\n",
    "==========\n",
    "\n",
    "This section defines the methods that:\n",
    "    (1). Read in the words, add padding, and assign each an integer\n",
    "    (2). Read in the tagss, add padding, and assign each a category of the form\n",
    "    [0,...,1,...0] etc.\n",
    "\n",
    "Section 2 will deal with creating the window and mini-batching\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    1.0. Utility Methods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_tokens(filename, col_val=-1):\n",
    "    # Col Values\n",
    "    # 0 - words\n",
    "    # 1 - POS\n",
    "    # 2 - tags\n",
    "    # -1 - for everything\n",
    "\n",
    "    with open(filename, 'rt', encoding='utf8') as csvfile:\n",
    "            r = csv.reader(csvfile, delimiter=' ')\n",
    "            words = np.transpose(np.array([x for x in list(r) if x != []])).astype(object)\n",
    "    # padding token '0'\n",
    "    print('reading' + filename)\n",
    "    if col_val!=-1:\n",
    "        words = words[col_val]\n",
    "    if col_val!=-1:\n",
    "        return [str(x).lower() for x in words]\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "def import_embeddings(filename):\n",
    "    words = {}\n",
    "    with open(filename, 'rt', encoding='utf8') as csvfile:\n",
    "        r = csv.reader(csvfile, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        for row in r:\n",
    "            words[row[0]] = row[1:301]\n",
    "    return words\n",
    "\n",
    "def _build_vocab(filename, ptb_filename, col_val):\n",
    "    # can be used for input vocab\n",
    "    conll_data = read_tokens(filename, col_val)\n",
    "    ptb_data = read_tokens(ptb_filename, col_val)\n",
    "    def replace_eos_ptb(token):\n",
    "        if token==\"<eos>\":\n",
    "            return \".\"\n",
    "        else:\n",
    "            return token\n",
    "    ptb_data = [replace_eos_ptb(token) for token in ptb_data]\n",
    "    data = np.concatenate((conll_data, ptb_data))\n",
    "    counter = collections.Counter(data)\n",
    "    # get rid of all words with frequency == 1\n",
    "    counter = {k: v for k, v in counter.items() if (v > 1)}\n",
    "    counter['<unk>'] = 10000\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return word_to_id\n",
    "\n",
    "def _build_vocab_embedding(filename, ptb_filename, padding_width, col_val, embedding):\n",
    "    # can be used for input vocab\n",
    "    conll_data = read_tokens(filename, col_val)\n",
    "    ptb_data = read_tokens(ptb_filename, col_val)\n",
    "    ptb_data = [replace_eos_ptb(token) for token in ptb_data]\n",
    "    data = np.concatenate((conll_data, ptb_data))\n",
    "    counter = collections.Counter(data)\n",
    "    # get rid of all words with frequency == 1\n",
    "    counter = {k: v for k, v in counter.items() if (v > 1) or (k not in embedding)}\n",
    "    counter['<unk>'] = 10000\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return word_to_id\n",
    "\n",
    "def _build_tags(filename, col_val):\n",
    "    # can be used for classifications and input vocab\n",
    "    data = read_tokens(filename, col_val)\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    tag_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return tag_to_id\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    1.1. Word Methods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id, ptb=False):\n",
    "    # assumes _build_vocab has been called first as is called word to id\n",
    "    data = read_tokens(filename, 0)\n",
    "    default_value = word_to_id['<unk>']\n",
    "    if ptb==True:\n",
    "        data = [replace_eos_ptb(s) for s in data]\n",
    "    return [word_to_id.get(word, default_value) for word in data]\n",
    "\n",
    "\"\"\"\n",
    "    1.2. tag Methods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _int_to_tag(tag_int, tag_vocab_size):\n",
    "    # creates the one-hot vector\n",
    "    a = np.empty(tag_vocab_size)\n",
    "    a.fill(0)\n",
    "    np.put(a, tag_int, 1)\n",
    "    return a\n",
    "\n",
    "\n",
    "def _seq_tag(tag_integers, tag_vocab_size):\n",
    "    # create the array of one-hot vectors for your sequence\n",
    "    return np.vstack(_int_to_tag(\n",
    "                     tag, tag_vocab_size) for tag in tag_integers)\n",
    "\n",
    "\n",
    "def _file_to_tag_classifications(filename, tag_to_id, col_val):\n",
    "    # assumes _build_vocab has been called first and is called tag to id\n",
    "    data = read_tokens(filename, col_val)\n",
    "    return [tag_to_id[tag] for tag in data]\n",
    "\n",
    "\n",
    "def raw_x_y_data(data_path, ptb_data_path, embedding=False, embedding_path=None):\n",
    "    train = \"train.txt\"\n",
    "    valid = \"validation.txt\"\n",
    "    train_valid = \"train_val_combined.txt\"\n",
    "    comb = \"all_combined.txt\"\n",
    "    test = \"test.txt\"\n",
    "    ptb = 'train.txt'\n",
    "\n",
    "    train_path = os.path.join(data_path, train)\n",
    "    valid_path = os.path.join(data_path, valid)\n",
    "    train_valid_path = os.path.join(data_path, train_valid)\n",
    "    comb_path = os.path.join(data_path, comb)\n",
    "    test_path = os.path.join(data_path, test)\n",
    "    ptb_path = os.path.join(ptb_data_path, ptb)\n",
    "\n",
    "    if os.path.exists(comb_path) != True:\n",
    "        print('writing combined')\n",
    "        test_data = pd.read_csv(test_path, sep= ' ',header=None)\n",
    "        train_data = pd.read_csv(train_path, sep= ' ',header=None)\n",
    "        valid_data = pd.read_csv(valid_path, sep= ' ', header=None)\n",
    "\n",
    "        comb = pd.concat([train_data,valid_data,test_data])\n",
    "        comb.to_csv(os.path.join(data_path,'critic_all_combined.txt'), sep=' ', index=False, header=False)\n",
    "\n",
    "    if os.path.exists(train_valid_path) != True:\n",
    "        print('writing combined')\n",
    "        valid_data = pd.read_csv(valid_path, sep= ' ',header=None)\n",
    "        train_data = pd.read_csv(train_path, sep= ' ',header=None)\n",
    "\n",
    "        comb = pd.concat([train_data,valid_data])\n",
    "        comb.to_csv(os.path.join(data_path,'critic_train_val_combined.txt'), sep=' ', index=False, header=False)\n",
    "\n",
    "\n",
    "    if embedding == True:\n",
    "        word_embedding_full = import_embeddings(embedding_path)\n",
    "        word_to_id = _build_vocab_embedding(comb_path, ptb_path, 0, 0, word_embedding_full)\n",
    "        id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "        ordered_vocab = [id_to_word[i] for i in range(len(id_to_word))]\n",
    "        word_embedding = [word_embedding_full.get(key.lower(), np.random.rand(300))\n",
    "                                   for key in ordered_vocab]\n",
    "    else:\n",
    "        word_to_id = _build_vocab(comb_path, ptb_path, 0, 0)\n",
    "        word_embedding = None\n",
    "\n",
    "    # use the full training set for building the target tags\n",
    "    pos_to_id = _build_tags(comb_path,1)\n",
    "    chunk_to_id = _build_tags(comb_path, 2)\n",
    "\n",
    "    word_data_t = _file_to_word_ids(train_path, word_to_id)\n",
    "    pos_data_t = _file_to_tag_classifications(train_path, pos_to_id, 1)\n",
    "    chunk_data_t = _file_to_tag_classifications(train_path, chunk_to_id, 2)\n",
    "\n",
    "    stop_id = word_to_id['.']\n",
    "    sentences = [list(group) for k, group in itertools.groupby(list(zip(word_data_t,pos_data_t,chunk_data_t)),lambda x: x[0]==stop_id)]\n",
    "    stop = sentences[1]\n",
    "    sentences = [sentences[i] for i in range(len(sentences)) if i%2==0]\n",
    "    for s in sentences:\n",
    "        s.append(stop[0])\n",
    "    sentences = np.transpose([list(zip(*sentence)) for sentence in sentences])\n",
    "    word_data_t = sentences[0]\n",
    "    pos_data_t = sentences[1]\n",
    "    chunk_data_t = sentences[2]\n",
    "\n",
    "    ptb_word_data = _file_to_word_ids(ptb_path, word_to_id, ptb=True)\n",
    "    ptb_pos_data = _file_to_tag_classifications(ptb_path, pos_to_id, 1)\n",
    "    ptb_chunk_data = _file_to_tag_classifications(ptb_path, chunk_to_id, 2)\n",
    "\n",
    "    sentences = [list(group) for k, group in itertools.groupby(list(zip(ptb_word_data,ptb_pos_data,ptb_chunk_data)),lambda x: x[0]==stop_id)]\n",
    "    stop = sentences[1]\n",
    "    sentences = [sentences[i] for i in range(len(sentences)) if i%2==0]\n",
    "    for s in sentences:\n",
    "        s.append(stop[0])\n",
    "    sentences = np.transpose([list(zip(*sentence)) for sentence in sentences])\n",
    "\n",
    "    ptb_word_data = sentences[0]\n",
    "    ptb_pos_data = sentences[1]\n",
    "    ptb_chunk_data = sentences[2]\n",
    "\n",
    "    word_data_v = _file_to_word_ids(valid_path, word_to_id)\n",
    "    pos_data_v = _file_to_tag_classifications(valid_path, pos_to_id, 1)\n",
    "    chunk_data_v = _file_to_tag_classifications(valid_path, chunk_to_id, 2)\n",
    "\n",
    "    sentences = [list(group) for k, group in itertools.groupby(list(zip(word_data_v,pos_data_v,chunk_data_v)),lambda x: x[0]==stop_id)]\n",
    "    stop = sentences[1]\n",
    "    sentences = [sentences[i] for i in range(len(sentences)) if i%2==0]\n",
    "    for s in sentences:\n",
    "        s.append(stop[0])\n",
    "    sentences = np.transpose([list(zip(*sentence)) for sentence in sentences])\n",
    "    word_data_v = sentences[0]\n",
    "    pos_data_v = sentences[1]\n",
    "    chunk_data_v = sentences[2]\n",
    "\n",
    "    word_data_c = _file_to_word_ids(train_valid_path, word_to_id)\n",
    "    pos_data_c = _file_to_tag_classifications(train_valid_path, pos_to_id, 1)\n",
    "    chunk_data_c = _file_to_tag_classifications(train_valid_path, chunk_to_id, 2)\n",
    "\n",
    "    sentences = [list(group) for k, group in itertools.groupby(list(zip(word_data_c,pos_data_c,chunk_data_c)),lambda x: x[0]==stop_id)]\n",
    "    stop = sentences[1]\n",
    "    sentences = [sentences[i] for i in range(len(sentences)) if i%2==0]\n",
    "    for s in sentences:\n",
    "        s.append(stop[0])\n",
    "    sentences = np.transpose([list(zip(*sentence)) for sentence in sentences])\n",
    "    word_data_c = sentences[0]\n",
    "    pos_data_c = sentences[1]\n",
    "    chunk_data_c = sentences[2]\n",
    "\n",
    "    word_data_test = _file_to_word_ids(test_path, word_to_id)\n",
    "    pos_data_test = _file_to_tag_classifications(test_path, pos_to_id, 1)\n",
    "    chunk_data_test = _file_to_tag_classifications(test_path, chunk_to_id, 2)\n",
    "\n",
    "    sentences = [list(group) for k, group in itertools.groupby(list(zip(word_data_test,pos_data_test,chunk_data_test)),lambda x: x[0]==stop_id)]\n",
    "    stop = sentences[1]\n",
    "    sentences = [sentences[i] for i in range(len(sentences)) if i%2==0]\n",
    "    for s in sentences:\n",
    "        s.append(stop[0])\n",
    "    sentences = np.transpose([list(zip(*sentence)) for sentence in sentences])\n",
    "    word_data_test = sentences[0]\n",
    "    pos_data_test = sentences[1]\n",
    "    chunk_data_test = sentences[2]\n",
    "\n",
    "    return word_data_t, pos_data_t, chunk_data_t, word_data_v, \\\n",
    "        pos_data_v, chunk_data_v, word_to_id, pos_to_id, chunk_to_id, \\\n",
    "        word_data_test, pos_data_test, chunk_data_test, word_data_c, \\\n",
    "        pos_data_c, chunk_data_c, ptb_word_data, ptb_pos_data, ptb_chunk_data, word_embedding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "============\n",
    "Section 2.\n",
    "============\n",
    "\n",
    "Here we want to feed in the raw data, batch-size, and window size\n",
    " and get back mini batches. These will be of size [batch_size, num_steps]\n",
    "\n",
    "Args:\n",
    "raw_words = the raw array of the word integers\n",
    "raw_tag = the raw array of the tag integers\n",
    "batch_size = batch size\n",
    "num_steps = the number of steps you are going to look back in your rnn\n",
    "tag_vocab_size = the size of the the number of tag tokens (\n",
    "    needed for transfer into the [0,...,1,...,0] format)\n",
    "\n",
    "Yields\n",
    "(x,y) - x the batch, y the tag tags\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_batches(raw_words, raw_pos, raw_chunk, batch_size, num_steps, pos_vocab_size,\n",
    "                   chunk_vocab_size, vocab_size, max_length, continuing=False):\n",
    "    \"\"\"Create those minibatches.\"\"\"\n",
    "\n",
    "    def _reshape_and_pad(tokens, max_length):\n",
    "        sentence_lengths = [len(sentence) for sentence in tokens]\n",
    "        sentences = []\n",
    "        for sentence in tokens:\n",
    "            if len(sentence) < max_length:\n",
    "                sentences.append(np.pad(sentence,(0,max_length-len(sentence)),mode='constant', constant_values=0))\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "        return sentences, sentence_lengths\n",
    "\n",
    "    \"\"\"\n",
    "    1. Prepare the input (word) data\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    1.1 Split the tokens at the full stops\n",
    "    \"\"\"\n",
    "    word_data, sentence_lengths = _reshape_and_pad(raw_words,  max_length)\n",
    "    pos_data, _ = _reshape_and_pad(raw_pos, max_length)\n",
    "    chunk_data, _ = _reshape_and_pad(raw_chunk, max_length)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Do the epoch thing and iterate\n",
    "    \"\"\"\n",
    "    data_len = len(raw_words)\n",
    "\n",
    "    # how many times do you iterate to reach the end of the epoch\n",
    "    epoch_size = (data_len // (batch_size)) + 1\n",
    "\n",
    "    additional_sentences_required = batch_size - (data_len % batch_size)\n",
    "\n",
    "    additional_sentences = np.zeros((additional_sentences_required,max_length))\n",
    "\n",
    "    word_data = np.vstack((word_data, additional_sentences))\n",
    "    pos_data = np.vstack((pos_data, additional_sentences))\n",
    "    chunk_data = np.vstack((chunk_data, additional_sentences))\n",
    "    sentence_lengths = np.concatenate((sentence_lengths,np.zeros((additional_sentences_required))))\n",
    "\n",
    "    if epoch_size == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    if continuing == False:\n",
    "        for i in range(epoch_size):\n",
    "            x = word_data[i*batch_size:(i+1)*batch_size]\n",
    "            y_pos = np.vstack(_seq_tag(pos_data[tag],\n",
    "                              pos_vocab_size) for tag in np.arange(i*batch_size,(i+1)*batch_size))\n",
    "            y_chunk = np.vstack(_seq_tag(chunk_data[tag],\n",
    "                                chunk_vocab_size) for tag in np.arange(i*batch_size,(i+1)*batch_size))\n",
    "\n",
    "            sentence_batch = sentence_lengths[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            # append for last batch for lm\n",
    "            if i == epoch_size-1:\n",
    "                y_lm = np.vstack(_seq_tag(word_data[tag],\n",
    "                                    vocab_size) for tag in np.concatenate((np.arange(i*batch_size+1,(i+1)*batch_size), [0])))\n",
    "            else:\n",
    "                y_lm = np.vstack(_seq_tag(word_data[tag],\n",
    "                                    vocab_size) for tag in np.arange(i*batch_size+1,(i+1)*batch_size+1))\n",
    "            y_pos = y_pos.astype(np.int32)\n",
    "            y_chunk = y_chunk.astype(np.int32)\n",
    "            y_lm = y_lm.astype(np.int32)\n",
    "            print(i)\n",
    "            yield (x, y_pos, y_chunk, y_lm, sentence_batch)\n",
    "    else:\n",
    "        i = 0\n",
    "        while i > -1:\n",
    "            x = word_data[i*batch_size:(i+1)*batch_size]\n",
    "            y_pos = np.vstack(_seq_tag(pos_data[tag],\n",
    "                              pos_vocab_size) for tag in np.arange(i*batch_size,(i+1)*batch_size))\n",
    "            y_chunk = np.vstack(_seq_tag(chunk_data[tag],\n",
    "                                chunk_vocab_size) for tag in np.arange(i*batch_size,(i+1)*batch_size))\n",
    "            \n",
    "            sentence_batch = sentence_lengths[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            # append for last batch for lm\n",
    "            if i == epoch_size-1:\n",
    "                y_lm = np.vstack(_seq_tag(word_data[tag],\n",
    "                                    vocab_size) for tag in np.concatenate((np.arange(i*batch_size+1,(i+1)*batch_size), [0])))\n",
    "            else:\n",
    "                y_lm = np.vstack(_seq_tag(word_data[tag],\n",
    "                                    vocab_size) for tag in np.arange(i*batch_size+1,(i+1)*batch_size+1))\n",
    "            y_pos = y_pos.astype(np.int32)\n",
    "            y_chunk = y_chunk.astype(np.int32)\n",
    "            y_lm = y_lm.astype(np.int32)\n",
    "            i = (i+1) % (epoch_size)\n",
    "            yield (x, y_pos, y_chunk, y_lm, sentence_batch)\n",
    "\n",
    "\n",
    "def _res_to_list(res, batch_size, to_id, w_length, sentence_lengths, to_str=False):\n",
    "    tmp = np.concatenate(res)\n",
    "    tmp = tmp[:w_length]\n",
    "    tmp = np.concatenate([tmp[i][:sentence_lengths[i]] for i in range(len(tmp))])\n",
    "    inv_dict = {v: k for k, v in to_id.items()}\n",
    "    if to_str:\n",
    "        result = np.array([inv_dict[x] for x in tmp])\n",
    "    return result\n",
    "\n",
    "def replace_eos_ptb(token):\n",
    "    if token==\"<eos>\":\n",
    "        return \".\"\n",
    "    else:\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading../../data/conll_toy/data/all_combined.txt\n",
      "reading../../data/conll_toy/data/train.txt\n",
      "reading../../data/conll_toy/data/all_combined.txt\n",
      "reading../../data/conll_toy/data/all_combined.txt\n",
      "reading../../data/conll_toy/data/train.txt\n",
      "reading../../data/conll_toy/data/train.txt\n",
      "reading../../data/conll_toy/data/train.txt\n",
      "reading../../data/conll_toy/data/train.txt\n",
      "reading../../data/conll_toy/data/train.txt\n",
      "reading../../data/conll_toy/data/train.txt\n",
      "reading../../data/conll_toy/data/validation.txt\n",
      "reading../../data/conll_toy/data/validation.txt\n",
      "reading../../data/conll_toy/data/validation.txt\n",
      "reading../../data/conll_toy/data/train_val_combined.txt\n",
      "reading../../data/conll_toy/data/train_val_combined.txt\n",
      "reading../../data/conll_toy/data/train_val_combined.txt\n",
      "reading../../data/conll_toy/data/test.txt\n",
      "reading../../data/conll_toy/data/test.txt\n",
      "reading../../data/conll_toy/data/test.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "dataset_path = \"../../data/conll_toy\"\n",
    "ptb_path = \"../../data/conll_toy\"\n",
    "save_path = \"../../data/outputs/test\"\n",
    "glove_path = '../../data/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "raw_data_path = dataset_path + '/data'\n",
    "raw_data = raw_x_y_data(\n",
    "    raw_data_path, ptb_path + '/data', True, glove_path)\n",
    "\n",
    "words_t, pos_t, chunk_t, words_v, \\\n",
    "    pos_v, chunk_v, word_to_id, pos_to_id, \\\n",
    "    chunk_to_id, words_test, pos_test, chunk_test, \\\n",
    "    words_c, pos_c, chunk_c, words_ptb, pos_ptb, chunk_ptb, word_embedding = raw_data\n",
    "\n",
    "num_pos_tags = len(pos_to_id)\n",
    "num_chunk_tags = len(chunk_to_id)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "train_lengths = [len(s) for s in words_t]\n",
    "validation_lengths = [len(s) for s in words_v]\n",
    "test_lengths = [len(s) for s in words_test]\n",
    "ptb_lengths = [len(s) for s in words_ptb]\n",
    "combined_lengths = [len(s) for s in words_c]\n",
    "\n",
    "max_length = np.max([np.max([len(s) for s in words_t]),\n",
    "                    np.max([len(s) for s in words_ptb]),\n",
    "                    np.max([len(s) for s in words_v]),\n",
    "                    np.max([len(s) for s in words_test])])\n",
    "\n",
    "epoch_length = (len(words_t) // 64) + 1\n",
    "\n",
    "conll_batches = create_batches(words_t, pos_t, chunk_t, 64,\n",
    "                        max_length, num_pos_tags, num_chunk_tags, vocab_size, max_length, continuing=True)\n",
    "cx = []\n",
    "cy_pos = []\n",
    "cy_chunk = []\n",
    "cy_lm = []\n",
    "\n",
    "for i in range(epoch_length):\n",
    "    (x, y_pos, y_chunk, y_lm, sentence_lengths) = next(conll_batches)\n",
    "    cx.append(x)\n",
    "    cy_pos.append(y_pos)\n",
    "    cy_chunk.append(y_chunk)\n",
    "    cy_lm.append(cy_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22.,  18.,  30.,  24.,  16.,  13.,  30.,  26.,   9.,  19.,  28.,\n",
       "        28.,  36.,  26.,  22.,  21.,  52.,  31.,  25.,  13.,  28.,  25.,\n",
       "        25.,  19.,  15.,  26.,  16.,  23.,  22.,  19.,  26.,  17.,   9.,\n",
       "        36.,  12.,   6.,  30.,  11.,  14.,   9.,  29.,  46.,  24.,  27.,\n",
       "        15.,  12.,  22.,   7.,  18.,  20.,   9.,  36.,  31.,   3.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.concatenate(cx)\n",
    "tmp = tmp[:len(train_lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = np.concatenate([tmp[i][:train_lengths[i]] for i in range(len(tmp))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2809"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 418,    6,    2, ...,   11, 1048,    3])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(words_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2809"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.concatenate(words_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['confidence', 'in', 'the', ..., 'at', 'twa', '.'], \n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_res_to_list(cx,64, word_to_id, len(train_lengths), train_lengths, to_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.concatenate(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ (418, 6, 2, 149, 19, 353, 147, 4, 421, 155, 369, 1090, 82, 80, 115, 9, 220, 1, 117, 9, 597, 738, 1, 906, 4, 134, 7, 377, 318, 17, 246, 8, 132, 15, 880, 1073, 3),\n",
       "       (210, 5, 2, 830, 598, 141, 15, 997, 386, 4, 7, 255, 161, 111, 37, 538, 4, 861, 7, 670, 6, 73, 84, 2, 208, 248, 3),\n",
       "       (63, 122, 531, 524, 902, 9, 73, 37, 188, 692, 26, 2, 210, 15, 789, 4, 949, 76, 58, 111, 722, 6, 47, 402, 341, 304, 55, 92, 3),\n",
       "       (41, 37, 158, 2, 631, 5, 2, 152, 515, 287, 4, 53, 350, 18, 4, 285, 14, 17, 39, 98, 266, 14, 306, 4, 552, 2, 149, 1, 197, 8, 226, 57, 68, 122, 416, 3),\n",
       "       (20, 2, 526, 9, 73, 5, 7, 294, 80, 918, 21, 427, 810, 33, 2, 145, 358, 1, 22, 13, 553, 207, 1, 257, 157, 225, 11, 958, 961, 376, 3),\n",
       "       (20, 82, 95, 19, 155, 294, 80, 193, 1, 95, 71, 27, 42, 676, 357, 5, 243, 1, 22, 382, 571, 303, 1, 157, 225, 9, 426, 1069, 1, 7, 237, 5, 426, 120, 868, 3),\n",
       "       (409, 9, 2, 80, 115, 567, 353, 1, 63, 137, 197, 204, 2, 172, 4, 134, 7, 427, 534, 318, 17, 2, 86, 473, 40, 312, 10, 1008, 40, 413, 108, 6, 2, 98, 218, 263, 9, 132, 3),\n",
       "       (2, 132, 108, 8, 2, 86, 898, 40, 292, 947, 6, 246, 21, 966, 99, 26, 2, 86, 1088, 40, 108, 5, 579, 229, 3),\n",
       "       (783, 986, 1, 360, 225, 11, 1040, 772, 138, 174, 1, 13, 95, 19, 151, 808, 12, 694, 15, 611, 97, 19, 576, 351, 4, 310, 156, 3),\n",
       "       (11, 2, 439, 166, 1, 34, 219, 408, 977, 29, 2, 1014, 9, 302, 1, 331, 276, 843, 217, 8, 387, 291, 643, 3),\n",
       "       (34, 512, 2, 98, 218, 108, 32, 400, 4, 99, 86, 1042, 40, 6, 220, 3),\n",
       "       (456, 1, 24, 207, 13, 34, 372, 12, 7, 454, 6, 766, 814, 917, 26, 97, 71, 262, 4, 7, 369, 293, 6, 302, 3),\n",
       "       (270, 28, 11, 184, 72, 759, 6, 156, 105, 132, 15, 886, 135, 1, 2, 108, 71, 400, 4, 23, 211, 23, 86, 307, 40, 3),\n",
       "       (24, 303, 1, 164, 66, 409, 7, 86, 307, 40, 98, 218, 292, 1, 809, 12, 500, 82, 2, 80, 115, 21, 585, 9, 73, 1, 2, 106, 180, 74, 875, 101, 272, 744, 32, 396, 4, 222, 91, 406, 5, 2, 836, 349, 638, 486, 3),\n",
       "       (1065, 1, 34, 382, 1, 20, 151, 128, 32, 396, 4, 159, 707, 2, 80, 115, 649, 7, 734, 385, 22, 6, 2, 149, 3),\n",
       "       (1054, 1, 683, 406, 33, 2, 148, 219, 408, 890, 3),\n",
       "       (6, 47, 402, 341, 304, 1, 24, 141, 663, 12, 7, 91, 403, 127, 27, 147, 23, 2, 680, 5, 2, 55, 116, 6, 290, 18, 96, 41, 286, 475, 761, 3),\n",
       "       (157, 350, 18, 21, 11, 39, 662, 306, 6, 345, 50, 3),\n",
       "       (63, 217, 1075, 172, 953, 150, 330, 74, 686, 12, 2, 157, 148, 19, 651, 12, 894, 3),\n",
       "       (2, 115, 134, 12, 296, 114, 899, 14, 6, 2, 431, 64, 17, 2, 230, 64, 8, 30, 52, 1077, 14, 17, 7, 43, 495, 3),\n",
       "       (41, 801, 28, 7, 1072, 14, 116, 6, 2, 230, 17, 2, 410, 64, 8, 7, 874, 14, 53, 17, 2, 230, 64, 5, 229, 3),\n",
       "       (24, 207, 13, 2, 172, 134, 2, 148, 20, 19, 264, 762, 168, 1, 22, 63, 999, 12, 101, 5, 2, 296, 423, 33, 851, 516, 60, 217, 291, 864, 405, 858, 5, 45, 901, 570, 3),\n",
       "       (696, 1, 2, 210, 37, 434, 16, 780, 12, 34, 19, 1024, 4, 53, 290, 18, 931, 82, 228, 4, 124, 371, 12, 7, 377, 403, 325, 421, 355, 8, 12, 73, 325, 74, 135, 91, 3),\n",
       "       (92, 1, 34, 590, 47, 779, 12, 2, 152, 20, 127, 239, 856, 2, 228, 669, 5, 161, 111, 4, 27, 991, 26, 57, 61, 660, 3),\n",
       "       (22, 122, 983, 95, 19, 211, 506, 73, 255, 11, 2, 823, 78, 60, 24, 141, 15, 190, 12, 18, 32, 27, 1067, 90, 82, 228, 3),\n",
       "       (8, 1, 44, 893, 1, 76, 91, 293, 6, 2, 152, 15, 1091, 71, 605, 182, 41, 190, 747, 682, 3),\n",
       "       (73, 30, 492, 606, 72, 395, 5, 7, 819, 5, 418, 6, 24, 141, 15, 190, 150, 3),\n",
       "       (6, 360, 59, 16, 260, 4, 10, 735, 8, 616, 102, 17, 10, 717, 8, 536, 102, 112, 92, 3),\n",
       "       (197, 941, 12, 82, 2, 149, 905, 101, 1010, 803, 102, 1, 2, 152, 32, 27, 287, 4, 53, 18, 4, 285, 14, 1, 124, 4, 1001, 76, 91, 135, 8, 371, 12, 2, 1005, 5, 161, 111, 219, 435, 3),\n",
       "       (150, 15, 68, 993, 2, 202, 324, 333, 6, 549, 59, 23, 627, 29, 608, 909, 3),\n",
       "       (226, 57, 1023, 13, 12, 2, 106, 68, 37, 311, 4, 892, 351, 17, 2, 990, 146, 57, 1, 176, 37, 962, 2, 68, 366, 203, 995, 1, 110, 2, 1087, 1041, 845, 910, 626, 45, 60, 797, 653, 3),\n",
       "       (106, 122, 490, 12, 6, 2, 337, 248, 2, 226, 57, 68, 32, 943, 49, 764, 252, 4, 746, 862, 1, 716, 7, 123, 908, 126, 9, 76, 395, 5, 161, 1082, 26, 36, 314, 489, 3),\n",
       "       (112, 6, 2, 58, 109, 59, 442, 1, 2, 202, 30, 348, 11, 547, 102, 1, 52, 17, 723, 102, 112, 92, 6, 58, 109, 3),\n",
       "       (2, 36, 106, 30, 66, 787, 742, 11, 799, 140, 1, 52, 17, 609, 140, 6, 58, 109, 112, 92, 3),\n",
       "       (6, 361, 33, 254, 1, 2, 36, 106, 1085, 9, 59, 11, 883, 140, 1, 52, 17, 150, 15, 361, 123, 5, 921, 140, 3),\n",
       "       (33, 2, 968, 57, 6, 58, 109, 1, 284, 9, 98, 904, 679, 11, 10, 915, 42, 288, 1, 52, 94, 274, 3),\n",
       "       (447, 600, 30, 7, 690, 1033, 25, 652, 3),\n",
       "       (6, 227, 59, 6, 847, 859, 254, 1, 284, 30, 348, 11, 10, 545, 42, 288, 3),\n",
       "       (104, 118, 233, 381, 322, 13, 16, 546, 4, 695, 89, 93, 665, 51, 9, 10, 289, 7, 67, 3),\n",
       "       (6, 42, 203, 760, 214, 4, 89, 93, 15, 143, 1, 104, 118, 13, 2, 499, 19, 954, 4, 2, 1011, 5, 7, 811, 363, 26, 151, 850, 60, 203, 1057, 3),\n",
       "       (2, 214, 1, 842, 4, 7, 235, 28, 2, 339, 8, 57, 300, 1, 13, 2, 510, 19, 66, 666, 1071, 778, 1039, 368, 3),\n",
       "       (42, 89, 93, 373, 260, 4, 497, 33, 2, 235, 3),\n",
       "       (2, 10, 575, 933, 920, 2, 31, 11, 29, 10, 784, 25, 3),\n",
       "       (89, 93, 1053, 37, 529, 54, 253, 3),\n",
       "       (49, 146, 196, 11, 10, 625, 1, 52, 10, 713, 1, 6, 200, 468, 59, 3),\n",
       "       (2, 31, 19, 7, 1050, 1, 1055, 1, 730, 5, 857, 75, 3),\n",
       "       (104, 118, 66, 13, 6, 2, 235, 12, 16, 335, 49, 1084, 6, 89, 93, 4, 820, 14, 3),\n",
       "       (16, 356, 969, 924, 89, 93, 163, 54, 1, 430, 704, 54, 483, 55, 92, 9, 10, 582, 4, 10, 798, 7, 67, 3),\n",
       "       (58, 786, 365, 654, 8, 244, 1034, 240, 104, 118, 233, 51, 1, 2, 343, 259, 380, 5, 104, 118, 233, 1031, 3),\n",
       "       (2, 343, 381, 380, 5, 2, 322, 19, 133, 329, 817, 51, 1, 42, 511, 201, 5, 133, 183, 453, 3),\n",
       "       (124, 133, 329, 8, 133, 183, 21, 249, 6, 950, 3),\n",
       "       (56, 18, 1, 525, 9, 256, 5, 2, 407, 272, 5, 250, 463, 26, 139, 1, 21, 659, 126, 1, 628, 1095, 8, 701, 4, 265, 162, 3),\n",
       "       (336, 1, 412, 8, 298, 69, 48, 316, 61, 65, 1, 261, 9, 41, 838, 35, 227, 107, 43, 1, 414, 90, 62, 8, 935, 308, 9, 56, 189, 3),\n",
       "       (481, 46, 416, 44, 204, 56, 18, 4, 116, 11, 184, 23, 826, 23, 162, 8, 876, 1058, 6, 2, 107, 137, 50, 3),\n",
       "       (12, 15, 7, 192, 342, 17, 1007, 50, 110, 56, 982, 30, 7, 984, 706, 9, 36, 528, 1, 896, 4, 818, 162, 8, 182, 36, 97, 45, 684, 457, 3),\n",
       "       (20, 308, 37, 957, 52, 28, 2, 484, 5, 1019, 569, 5, 56, 85, 1, 8, 18, 21, 383, 4, 474, 52, 22, 11, 7, 61, 20, 123, 4, 35, 599, 45, 60, 2, 162, 61, 1, 22, 13, 873, 785, 1, 397, 5, 315, 11, 976, 556, 187, 3),\n",
       "       (46, 971, 1080, 26, 283, 317, 865, 13, 44, 204, 39, 209, 1, 844, 8, 831, 62, 4, 116, 29, 332, 14, 41, 43, 3),\n",
       "       (99, 170, 14, 5, 2, 487, 46, 557, 147, 39, 209, 62, 4, 615, 1, 503, 28, 479, 14, 164, 83, 572, 4, 56, 189, 4, 405, 62, 6, 208, 50, 3),\n",
       "       (20, 41, 19, 2, 410, 43, 366, 85, 139, 6, 425, 12, 100, 48, 83, 142, 7, 673, 8, 741, 711, 6, 1038, 85, 18, 1, 22, 13, 913, 367, 1, 7, 85, 315, 658, 11, 283, 317, 6, 872, 3),\n",
       "       (2, 139, 5, 555, 8, 336, 69, 12, 639, 6, 425, 560, 46, 4, 932, 9, 85, 3),\n",
       "       (77, 71, 458, 39, 186, 45, 960, 1, 295, 4, 678, 44, 79, 790, 4, 320, 3),\n",
       "       (46, 595, 29, 10, 739, 40, 17, 39, 855, 1, 907, 205, 8, 319, 62, 1, 4, 29, 10, 1051, 40, 1, 35, 29, 849, 14, 5, 622, 200, 477, 1, 145, 17, 781, 14, 5, 562, 6, 689, 3),\n",
       "       (63, 28, 101, 5, 2, 620, 568, 126, 5, 2, 209, 867, 1, 832, 62, 21, 130, 4, 27, 566, 712, 6, 90, 56, 18, 3),\n",
       "       (20, 46, 21, 715, 751, 2, 992, 15, 84, 1, 504, 22, 13, 24, 367, 3),\n",
       "       (20, 46, 180, 74, 27, 198, 4, 501, 9, 866, 267, 23, 44, 48, 9, 2, 55, 345, 35, 279, 50, 3),\n",
       "       (189, 18, 180, 74, 27, 42, 965, 9, 749, 181, 65, 6, 78, 523, 5, 2, 148, 3),\n",
       "       (22, 244, 973, 1, 7, 441, 11, 970, 731, 211, 51, 1, 642, 1, 889, 1, 13, 20, 100, 1012, 879, 206, 2, 700, 5, 139, 6, 703, 451, 3),\n",
       "       (356, 100, 21, 383, 4, 222, 131, 672, 65, 23, 77, 1044, 186, 1, 242, 90, 265, 62, 8, 242, 45, 9, 641, 3),\n",
       "       (232, 948, 222, 77, 1035, 4, 1028, 72, 5, 2, 465, 634, 12, 1070, 592, 3),\n",
       "       (22, 239, 773, 372, 12, 2, 493, 768, 21, 84, 9, 46, 3),\n",
       "       (20, 95, 15, 264, 7, 357, 5, 243, 33, 18, 6, 124, 319, 8, 205, 1, 22, 13, 1074, 900, 1, 632, 6, 85, 11, 800, 376, 5, 788, 3),\n",
       "       (362, 69, 1, 176, 668, 2, 56, 5, 998, 46, 6, 236, 205, 827, 1, 878, 378, 7, 1093, 14, 61, 53, 1015, 55, 404, 3),\n",
       "       (2, 77, 79, 792, 1026, 9, 68, 67, 3),\n",
       "       (1049, 65, 21, 130, 4, 27, 587, 26, 602, 841, 346, 8, 594, 250, 9, 56, 17, 928, 3),\n",
       "       (42, 373, 11, 305, 428, 51, 1, 7, 708, 354, 1, 313, 1, 362, 951, 1, 13, 61, 871, 6, 12, 97, 37, 311, 4, 20, 882, 3),\n",
       "       (22, 305, 428, 399, 4, 231, 49, 18, 537, 14, 112, 41, 43, 35, 227, 107, 43, 1, 8, 11, 184, 81, 496, 48, 316, 466, 65, 3),\n",
       "       (807, 21, 20, 327, 4, 964, 1027, 12, 44, 591, 4, 527, 2, 647, 1, 804, 29, 68, 67, 8, 159, 9, 90, 18, 1, 22, 13, 927, 282, 1, 42, 937, 11, 375, 791, 3),\n",
       "       (8, 20, 46, 21, 1092, 2, 732, 12, 44, 48, 952, 128, 699, 650, 498, 155, 23, 101, 23, 44, 127, 1, 22, 34, 13, 3),\n",
       "       (298, 77, 758, 39, 18, 9, 36, 75, 846, 963, 2, 271, 4, 411, 26, 29, 94, 14, 96, 41, 286, 3),\n",
       "       (8, 299, 388, 175, 13, 16, 399, 4, 310, 49, 18, 7, 91, 326, 14, 84, 2, 107, 81, 50, 3),\n",
       "       (142, 61, 65, 20, 32, 53, 2, 309, 181, 5, 36, 75, 8, 748, 145, 2, 61, 5, 53, 5, 36, 156, 1, 22, 13, 273, 1094, 1, 7, 257, 179, 129, 5, 277, 388, 138, 765, 646, 763, 51, 1, 2, 36, 750, 201, 5, 521, 277, 1032, 5, 299, 3),\n",
       "       (194, 69, 238, 394, 903, 1, 142, 23, 975, 1, 770, 1, 645, 8, 535, 1046, 1, 48, 188, 198, 4, 53, 39, 18, 6, 2, 55, 212, 5, 50, 3),\n",
       "       (72, 394, 412, 18, 48, 158, 20, 268, 14, 4, 332, 14, 6, 2, 208, 137, 390, 1, 22, 13, 375, 15, 24, 282, 3),\n",
       "       (8, 194, 175, 238, 379, 21, 66, 327, 4, 231, 39, 18, 3),\n",
       "       (77, 335, 18, 45, 60, 170, 14, 6, 2, 767, 596, 664, 2, 36, 8, 563, 55, 220, 1, 623, 4, 697, 956, 18, 4, 96, 346, 3),\n",
       "       (194, 175, 393, 6, 2, 271, 449, 4, 231, 18, 33, 379, 238, 36, 156, 4, 411, 29, 170, 14, 1, 518, 107, 404, 3),\n",
       "       (301, 415, 51, 13, 16, 752, 7, 854, 389, 28, 2, 339, 8, 57, 300, 9, 7, 417, 420, 5, 508, 25, 163, 54, 3),\n",
       "       (2, 980, 693, 1, 805, 31, 13, 16, 113, 48, 705, 25, 163, 54, 253, 105, 2, 420, 3),\n",
       "       (2, 539, 8, 718, 31, 13, 753, 714, 387, 612, 32, 262, 2, 1056, 3),\n",
       "       (443, 17, 2, 448, 32, 27, 1016, 9, 588, 8, 816, 581, 1, 23, 223, 23, 9, 2, 1002, 301, 415, 912, 8, 891, 354, 3),\n",
       "       (280, 70, 834, 84, 7, 214, 17, 47, 154, 593, 125, 33, 1003, 9, 657, 3),\n",
       "       (24, 70, 1006, 2, 1029, 30, 944, 167, 34, 509, 12, 47, 154, 347, 4, 437, 926, 5, 125, 9, 432, 323, 3),\n",
       "       (897, 4, 923, 2, 154, 4, 342, 47, 384, 1, 34, 423, 4, 7, 20, 31, 88, 22, 9, 7, 914, 3),\n",
       "       (11, 2, 261, 166, 1, 24, 70, 825, 7, 1062, 640, 6, 7, 419, 725, 607, 34, 1018, 3),\n",
       "       (105, 2, 450, 258, 5, 2, 88, 470, 340, 1, 2, 87, 5, 2, 1061, 13, 169, 20, 159, 558, 8, 564, 144, 119, 251, 3),\n",
       "       (100, 177, 1086, 374, 23, 232, 159, 757, 1, 35, 100, 177, 1081, 839, 2, 614, 3),\n",
       "       (22, 151, 911, 35, 885, 1098, 79, 887, 3),\n",
       "       (2, 99, 981, 821, 79, 7, 212, 5, 195, 942, 1, 128, 5, 1037, 1064, 24, 70, 15, 782, 637, 8, 113, 561, 320, 6, 76, 936, 724, 4, 532, 2, 88, 2, 584, 685, 3),\n",
       "       (84, 7, 542, 5, 559, 1, 24, 70, 617, 47, 959, 3),\n",
       "       (34, 729, 29, 94, 1000, 3),\n",
       "       (110, 34, 30, 485, 1, 2, 88, 258, 494, 245, 374, 1, 794, 2, 87, 13, 44, 113, 919, 4, 216, 47, 154, 15, 358, 8, 860, 4, 1066, 3),\n",
       "       (2, 87, 1076, 24, 70, 7, 384, 938, 81, 601, 3),\n",
       "       (280, 70, 19, 7, 852, 813, 1, 63, 2, 1083, 940, 19, 131, 3),\n",
       "       (16, 251, 11, 972, 160, 6, 720, 618, 3),\n",
       "       (2, 88, 19, 756, 2, 121, 370, 391, 1, 35, 352, 20, 828, 1, 22, 8, 16, 19, 130, 4, 216, 7, 212, 5, 604, 191, 7, 43, 3),\n",
       "       (745, 72, 822, 5, 41, 709, 8, 16, 71, 27, 736, 355, 321, 11, 314, 505, 6, 551, 1, 2, 452, 8, 1017, 334, 644, 5, 583, 6, 648, 1, 7, 259, 755, 877, 6, 1025, 1, 916, 1, 35, 7, 193, 5, 78, 69, 3),\n",
       "       (472, 719, 21, 589, 6, 7, 769, 58, 930, 6, 2, 103, 796, 169, 2, 116, 5, 119, 398, 1009, 103, 117, 153, 3),\n",
       "       (829, 103, 117, 153, 19, 513, 321, 6, 137, 69, 178, 806, 289, 4, 661, 178, 16, 19, 128, 5, 2, 520, 633, 955, 6, 97, 3),\n",
       "       (6, 2, 337, 407, 7, 881, 5, 775, 69, 21, 130, 4, 543, 16, 3),\n",
       "       (103, 117, 153, 370, 4, 121, 9, 7, 848, 5, 323, 3),\n",
       "       (16, 1060, 1068, 17, 573, 476, 8, 987, 1, 28, 206, 12, 281, 9, 491, 1004, 62, 8, 522, 417, 710, 3),\n",
       "       (16, 502, 4, 687, 126, 721, 3),\n",
       "       (16, 65, 1079, 386, 4, 2, 31, 1, 28, 206, 12, 281, 9, 922, 8, 677, 240, 3),\n",
       "       (119, 359, 996, 121, 674, 330, 4, 777, 103, 117, 153, 636, 754, 21, 974, 727, 869, 169, 925, 3),\n",
       "       (182, 1030, 232, 48, 7, 168, 195, 550, 3),\n",
       "       (16, 359, 27, 198, 4, 978, 256, 5, 2, 338, 12, 127, 239, 27, 1045, 6, 2, 740, 26, 743, 8, 39, 688, 1, 726, 2, 31, 88, 35, 979, 32, 27, 1043, 28, 191, 3),\n",
       "       (11, 824, 1, 2, 195, 111, 580, 391, 177, 216, 99, 29, 94, 191, 7, 43, 167, 2, 934, 5, 2, 245, 467, 5, 338, 21, 985, 11, 96, 1089, 3),\n",
       "       (11, 1048, 3)], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ (418, 6, 2, 149, 19, 353, 147, 4, 421, 155, 369, 1090, 82, 80, 115, 9, 220, 1, 117, 9, 597, 738, 1, 906, 4, 134, 7, 377, 318, 17, 246, 8, 132, 15, 880, 1073, 3),\n",
       "       (210, 5, 2, 830, 598, 141, 15, 997, 386, 4, 7, 255, 161, 111, 37, 538, 4, 861, 7, 670, 6, 73, 84, 2, 208, 248, 3),\n",
       "       (63, 122, 531, 524, 902, 9, 73, 37, 188, 692, 26, 2, 210, 15, 789, 4, 949, 76, 58, 111, 722, 6, 47, 402, 341, 304, 55, 92, 3),\n",
       "       (41, 37, 158, 2, 631, 5, 2, 152, 515, 287, 4, 53, 350, 18, 4, 285, 14, 17, 39, 98, 266, 14, 306, 4, 552, 2, 149, 1, 197, 8, 226, 57, 68, 122, 416, 3),\n",
       "       (20, 2, 526, 9, 73, 5, 7, 294, 80, 918, 21, 427, 810, 33, 2, 145, 358, 1, 22, 13, 553, 207, 1, 257, 157, 225, 11, 958, 961, 376, 3),\n",
       "       (20, 82, 95, 19, 155, 294, 80, 193, 1, 95, 71, 27, 42, 676, 357, 5, 243, 1, 22, 382, 571, 303, 1, 157, 225, 9, 426, 1069, 1, 7, 237, 5, 426, 120, 868, 3),\n",
       "       (409, 9, 2, 80, 115, 567, 353, 1, 63, 137, 197, 204, 2, 172, 4, 134, 7, 427, 534, 318, 17, 2, 86, 473, 40, 312, 10, 1008, 40, 413, 108, 6, 2, 98, 218, 263, 9, 132, 3),\n",
       "       (2, 132, 108, 8, 2, 86, 898, 40, 292, 947, 6, 246, 21, 966, 99, 26, 2, 86, 1088, 40, 108, 5, 579, 229, 3),\n",
       "       (783, 986, 1, 360, 225, 11, 1040, 772, 138, 174, 1, 13, 95, 19, 151, 808, 12, 694, 15, 611, 97, 19, 576, 351, 4, 310, 156, 3),\n",
       "       (11, 2, 439, 166, 1, 34, 219, 408, 977, 29, 2, 1014, 9, 302, 1, 331, 276, 843, 217, 8, 387, 291, 643, 3),\n",
       "       (34, 512, 2, 98, 218, 108, 32, 400, 4, 99, 86, 1042, 40, 6, 220, 3),\n",
       "       (456, 1, 24, 207, 13, 34, 372, 12, 7, 454, 6, 766, 814, 917, 26, 97, 71, 262, 4, 7, 369, 293, 6, 302, 3),\n",
       "       (270, 28, 11, 184, 72, 759, 6, 156, 105, 132, 15, 886, 135, 1, 2, 108, 71, 400, 4, 23, 211, 23, 86, 307, 40, 3),\n",
       "       (24, 303, 1, 164, 66, 409, 7, 86, 307, 40, 98, 218, 292, 1, 809, 12, 500, 82, 2, 80, 115, 21, 585, 9, 73, 1, 2, 106, 180, 74, 875, 101, 272, 744, 32, 396, 4, 222, 91, 406, 5, 2, 836, 349, 638, 486, 3),\n",
       "       (1065, 1, 34, 382, 1, 20, 151, 128, 32, 396, 4, 159, 707, 2, 80, 115, 649, 7, 734, 385, 22, 6, 2, 149, 3),\n",
       "       (1054, 1, 683, 406, 33, 2, 148, 219, 408, 890, 3),\n",
       "       (6, 47, 402, 341, 304, 1, 24, 141, 663, 12, 7, 91, 403, 127, 27, 147, 23, 2, 680, 5, 2, 55, 116, 6, 290, 18, 96, 41, 286, 475, 761, 3),\n",
       "       (157, 350, 18, 21, 11, 39, 662, 306, 6, 345, 50, 3),\n",
       "       (63, 217, 1075, 172, 953, 150, 330, 74, 686, 12, 2, 157, 148, 19, 651, 12, 894, 3),\n",
       "       (2, 115, 134, 12, 296, 114, 899, 14, 6, 2, 431, 64, 17, 2, 230, 64, 8, 30, 52, 1077, 14, 17, 7, 43, 495, 3),\n",
       "       (41, 801, 28, 7, 1072, 14, 116, 6, 2, 230, 17, 2, 410, 64, 8, 7, 874, 14, 53, 17, 2, 230, 64, 5, 229, 3),\n",
       "       (24, 207, 13, 2, 172, 134, 2, 148, 20, 19, 264, 762, 168, 1, 22, 63, 999, 12, 101, 5, 2, 296, 423, 33, 851, 516, 60, 217, 291, 864, 405, 858, 5, 45, 901, 570, 3),\n",
       "       (696, 1, 2, 210, 37, 434, 16, 780, 12, 34, 19, 1024, 4, 53, 290, 18, 931, 82, 228, 4, 124, 371, 12, 7, 377, 403, 325, 421, 355, 8, 12, 73, 325, 74, 135, 91, 3),\n",
       "       (92, 1, 34, 590, 47, 779, 12, 2, 152, 20, 127, 239, 856, 2, 228, 669, 5, 161, 111, 4, 27, 991, 26, 57, 61, 660, 3),\n",
       "       (22, 122, 983, 95, 19, 211, 506, 73, 255, 11, 2, 823, 78, 60, 24, 141, 15, 190, 12, 18, 32, 27, 1067, 90, 82, 228, 3),\n",
       "       (8, 1, 44, 893, 1, 76, 91, 293, 6, 2, 152, 15, 1091, 71, 605, 182, 41, 190, 747, 682, 3),\n",
       "       (73, 30, 492, 606, 72, 395, 5, 7, 819, 5, 418, 6, 24, 141, 15, 190, 150, 3),\n",
       "       (6, 360, 59, 16, 260, 4, 10, 735, 8, 616, 102, 17, 10, 717, 8, 536, 102, 112, 92, 3),\n",
       "       (197, 941, 12, 82, 2, 149, 905, 101, 1010, 803, 102, 1, 2, 152, 32, 27, 287, 4, 53, 18, 4, 285, 14, 1, 124, 4, 1001, 76, 91, 135, 8, 371, 12, 2, 1005, 5, 161, 111, 219, 435, 3),\n",
       "       (150, 15, 68, 993, 2, 202, 324, 333, 6, 549, 59, 23, 627, 29, 608, 909, 3),\n",
       "       (226, 57, 1023, 13, 12, 2, 106, 68, 37, 311, 4, 892, 351, 17, 2, 990, 146, 57, 1, 176, 37, 962, 2, 68, 366, 203, 995, 1, 110, 2, 1087, 1041, 845, 910, 626, 45, 60, 797, 653, 3),\n",
       "       (106, 122, 490, 12, 6, 2, 337, 248, 2, 226, 57, 68, 32, 943, 49, 764, 252, 4, 746, 862, 1, 716, 7, 123, 908, 126, 9, 76, 395, 5, 161, 1082, 26, 36, 314, 489, 3),\n",
       "       (112, 6, 2, 58, 109, 59, 442, 1, 2, 202, 30, 348, 11, 547, 102, 1, 52, 17, 723, 102, 112, 92, 6, 58, 109, 3),\n",
       "       (2, 36, 106, 30, 66, 787, 742, 11, 799, 140, 1, 52, 17, 609, 140, 6, 58, 109, 112, 92, 3),\n",
       "       (6, 361, 33, 254, 1, 2, 36, 106, 1085, 9, 59, 11, 883, 140, 1, 52, 17, 150, 15, 361, 123, 5, 921, 140, 3),\n",
       "       (33, 2, 968, 57, 6, 58, 109, 1, 284, 9, 98, 904, 679, 11, 10, 915, 42, 288, 1, 52, 94, 274, 3),\n",
       "       (447, 600, 30, 7, 690, 1033, 25, 652, 3),\n",
       "       (6, 227, 59, 6, 847, 859, 254, 1, 284, 30, 348, 11, 10, 545, 42, 288, 3),\n",
       "       (104, 118, 233, 381, 322, 13, 16, 546, 4, 695, 89, 93, 665, 51, 9, 10, 289, 7, 67, 3),\n",
       "       (6, 42, 203, 760, 214, 4, 89, 93, 15, 143, 1, 104, 118, 13, 2, 499, 19, 954, 4, 2, 1011, 5, 7, 811, 363, 26, 151, 850, 60, 203, 1057, 3),\n",
       "       (2, 214, 1, 842, 4, 7, 235, 28, 2, 339, 8, 57, 300, 1, 13, 2, 510, 19, 66, 666, 1071, 778, 1039, 368, 3),\n",
       "       (42, 89, 93, 373, 260, 4, 497, 33, 2, 235, 3),\n",
       "       (2, 10, 575, 933, 920, 2, 31, 11, 29, 10, 784, 25, 3),\n",
       "       (89, 93, 1053, 37, 529, 54, 253, 3),\n",
       "       (49, 146, 196, 11, 10, 625, 1, 52, 10, 713, 1, 6, 200, 468, 59, 3),\n",
       "       (2, 31, 19, 7, 1050, 1, 1055, 1, 730, 5, 857, 75, 3),\n",
       "       (104, 118, 66, 13, 6, 2, 235, 12, 16, 335, 49, 1084, 6, 89, 93, 4, 820, 14, 3),\n",
       "       (16, 356, 969, 924, 89, 93, 163, 54, 1, 430, 704, 54, 483, 55, 92, 9, 10, 582, 4, 10, 798, 7, 67, 3),\n",
       "       (58, 786, 365, 654, 8, 244, 1034, 240, 104, 118, 233, 51, 1, 2, 343, 259, 380, 5, 104, 118, 233, 1031, 3),\n",
       "       (2, 343, 381, 380, 5, 2, 322, 19, 133, 329, 817, 51, 1, 42, 511, 201, 5, 133, 183, 453, 3),\n",
       "       (124, 133, 329, 8, 133, 183, 21, 249, 6, 950, 3),\n",
       "       (56, 18, 1, 525, 9, 256, 5, 2, 407, 272, 5, 250, 463, 26, 139, 1, 21, 659, 126, 1, 628, 1095, 8, 701, 4, 265, 162, 3),\n",
       "       (336, 1, 412, 8, 298, 69, 48, 316, 61, 65, 1, 261, 9, 41, 838, 35, 227, 107, 43, 1, 414, 90, 62, 8, 935, 308, 9, 56, 189, 3),\n",
       "       (481, 46, 416, 44, 204, 56, 18, 4, 116, 11, 184, 23, 826, 23, 162, 8, 876, 1058, 6, 2, 107, 137, 50, 3),\n",
       "       (12, 15, 7, 192, 342, 17, 1007, 50, 110, 56, 982, 30, 7, 984, 706, 9, 36, 528, 1, 896, 4, 818, 162, 8, 182, 36, 97, 45, 684, 457, 3),\n",
       "       (20, 308, 37, 957, 52, 28, 2, 484, 5, 1019, 569, 5, 56, 85, 1, 8, 18, 21, 383, 4, 474, 52, 22, 11, 7, 61, 20, 123, 4, 35, 599, 45, 60, 2, 162, 61, 1, 22, 13, 873, 785, 1, 397, 5, 315, 11, 976, 556, 187, 3),\n",
       "       (46, 971, 1080, 26, 283, 317, 865, 13, 44, 204, 39, 209, 1, 844, 8, 831, 62, 4, 116, 29, 332, 14, 41, 43, 3),\n",
       "       (99, 170, 14, 5, 2, 487, 46, 557, 147, 39, 209, 62, 4, 615, 1, 503, 28, 479, 14, 164, 83, 572, 4, 56, 189, 4, 405, 62, 6, 208, 50, 3),\n",
       "       (20, 41, 19, 2, 410, 43, 366, 85, 139, 6, 425, 12, 100, 48, 83, 142, 7, 673, 8, 741, 711, 6, 1038, 85, 18, 1, 22, 13, 913, 367, 1, 7, 85, 315, 658, 11, 283, 317, 6, 872, 3),\n",
       "       (2, 139, 5, 555, 8, 336, 69, 12, 639, 6, 425, 560, 46, 4, 932, 9, 85, 3),\n",
       "       (77, 71, 458, 39, 186, 45, 960, 1, 295, 4, 678, 44, 79, 790, 4, 320, 3),\n",
       "       (46, 595, 29, 10, 739, 40, 17, 39, 855, 1, 907, 205, 8, 319, 62, 1, 4, 29, 10, 1051, 40, 1, 35, 29, 849, 14, 5, 622, 200, 477, 1, 145, 17, 781, 14, 5, 562, 6, 689, 3),\n",
       "       (63, 28, 101, 5, 2, 620, 568, 126, 5, 2, 209, 867, 1, 832, 62, 21, 130, 4, 27, 566, 712, 6, 90, 56, 18, 3),\n",
       "       (20, 46, 21, 715, 751, 2, 992, 15, 84, 1, 504, 22, 13, 24, 367, 3)], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_t[0:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
