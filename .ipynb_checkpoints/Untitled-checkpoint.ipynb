{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conll POS LSTM \n",
    "### Module & Data-reader Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "from tensorflow.models.rnn import rnn\n",
    "import conll_pos_reader as reader\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this works\n",
    "\n",
    "Building a tensorflow model have three basic parts (1) specify the model, (2) load in the data (3) specify how the model should run. \n",
    "\n",
    "### Specify the model\n",
    "\n",
    "Tensorflow works by specifying a model ('graph'), and then running this within a 'session'. The model has three parts to it:\n",
    "\n",
    "- Inference. This basically defines the computations within the neural net. Tensorflow comes with a series of built-in functions that contain the forward, backward and update passes. You use these to construct your net.\n",
    "\n",
    "- Loss. You need to define a way to calculate the loss associated with a pass through the net.\n",
    "\n",
    "- Optimiser. This is the way that you update the model parameters. Most popular is stochastic gradient descent.\n",
    "\n",
    "We are going to want to be able to use the forward-pass on the model to make predictions on a validation set. I.e. we're going to want to keep the (1) inference and (2) loss part of the model, but not call the optimiser on the validation (and test sets). \n",
    "\n",
    "For this reason we're going to create a model class, to which we can pass a parameter \"training\", to use with validation and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conll_POS_Model(object):\n",
    "    def __init__(self, is_training, pos_vocab):\n",
    "        \"\"\"\n",
    "        The init function takes the is_training parameter (for the validation sets), and the pos_vocab size.\n",
    "        The POS_vocab size is the number of possible POS tags. This is for shaping the size of the output layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Configuration.\n",
    "        A model has a fair number of configuration variables, defined here.\n",
    "        - init_scale - the initial scale of the weights\n",
    "        - learning_rate - the initial value of the learning rate\n",
    "        - max_grad_norm - the maximum permissible norm of the gradient\n",
    "        - num_layers - the number of LSTM layers\n",
    "        - num_steps - the number of unrolled steps of LSTM\n",
    "        - hidden_size - the number of LSTM units\n",
    "        - max_epoch - the number of epochs trained with the initial learning rate\n",
    "        - max_max_epoch - the total number of epochs for training\n",
    "        - keep_prob - the probability of keeping weights in the dropout layer\n",
    "        - lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "        - batch_size - the batch size\n",
    "        \"\"\"\n",
    "        \n",
    "        self.init_scale = init_scale = 0.1\n",
    "        self.learning_rate = learning_rate = 1.0\n",
    "        self.max_grad_norm = max_grad_norm = 5\n",
    "        self.num_layers = num_layers = 2\n",
    "        self.num_steps = num_steps = 10\n",
    "        self.hidden_size = hidden_size = 200 # update to size\n",
    "        self.max_epoch = max_epoch = 4 \n",
    "        self.max_max_epoch = max_max_epoch = 13\n",
    "        self.keep_prob = keep_prob = 1.0\n",
    "        self.lr_decay = lr_decay = 0.5\n",
    "        self.batch_size = batch_size = 20\n",
    "        self.vocab_size = vocab_size = 20000\n",
    "        \n",
    "        # ======================================\n",
    "        # 1. Inference\n",
    "        # ======================================\n",
    "        \n",
    "\n",
    "        # create the placeholders for inputs/outpus. Basically, this tells the graph how big the inputs\n",
    "        # are going to be and allocates space to them.\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # add exit size - NB. make sure to convert one-hot back to integer\n",
    "        self._targets = tf.placeholder(tf.float32, [batch_size, num_steps])\n",
    "        \n",
    "        \n",
    "        ## here we create the lstm cells - built in tensorflow\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n",
    "        \n",
    "        ## here we use our is_training parameter to only apply dropout when training\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = rnn_cell.DropoutWrapper(\n",
    "                lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        \n",
    "        ## we can create a multi-layer lstm super-easily using multiRNNcell\n",
    "        cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        \n",
    "        ## initialise variables. Note this doesn't actually initialise until we're in a session\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        ## choise to use cpu or gpu - not needed on mac but kept in if anyone else is using this\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            ## we're going to embed our words as vectors\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size])\n",
    "            # N.B. - need to check what this is doing!!!!!!\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "        \n",
    "        ## again we've got our training flag for the dropout\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        # outputs = []\n",
    "        # output = []\n",
    "        # state = self._initial_state\n",
    "        # with tf.variable_scope(\"RNN\"):\n",
    "        #     for time_step in range(num_steps):\n",
    "        #         if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "        #         (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "        #         outputs.append(cell_output)\n",
    "        #         output = cell_output\n",
    "\n",
    "        \n",
    "        # NB Need to check this code.\n",
    "        inputs = [tf.squeeze(input_, [1])\n",
    "                  for input_ in tf.split(1, num_steps, inputs)]\n",
    "        outputs, state = rnn.rnn(cell,\n",
    "                                  inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        \n",
    "        # ====================================\n",
    "        # 2. Specifying the Loss Function\n",
    "        # ====================================\n",
    "        \n",
    "        # get the weights\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        # do the logit calculation\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        pdb.set_trace()\n",
    "        # calculate the Loss\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, self._targets))\n",
    "        \n",
    "        # update the parameters of the object\n",
    "        self._cost = cost = loss\n",
    "        self._final_state = state\n",
    "        \n",
    "        # =====================================\n",
    "        # 3. Specifying the Optimiser\n",
    "        # =====================================\n",
    "        \n",
    "        \n",
    "        # flag for the training part\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # get the trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        # calculate gradients and clip\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        # specify your optimiser\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        \n",
    "        # do the updates\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
